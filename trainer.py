"""
è®­ç»ƒæ¨¡å— - ä½¿ç”¨ç»Ÿä¸€é…ç½®è¿›è¡Œæ¨¡å‹è®­ç»ƒ
"""

import os
import json
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

# è®¾ç½®æ—¥å¿—å™¨
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è®­ç»ƒç›¸å…³åº“
try:
    import torch
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM,
        TrainingArguments, Trainer,
        DataCollatorForSeq2Seq
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥ transformers å’Œç›¸å…³ä¾èµ–")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    logger.warning(f"âš ï¸  transformers æœªå®‰è£…: {e}")
    # åˆ›å»ºå ä½ç¬¦ç±»é¿å…å¯¼å…¥é”™è¯¯
    class AutoTokenizer:
        @staticmethod
        def from_pretrained(*args, **kwargs):
            raise ImportError("transformers æœªå®‰è£…")
    
    class AutoModelForCausalLM:
        @staticmethod
        def from_pretrained(*args, **kwargs):
            raise ImportError("transformers æœªå®‰è£…")

from config import ModelConfig, TrainingConfig, DataConfig
from data_manager import DataManager
from evaluation import ModelEvaluator

class SFTTrainer:
    """ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨ - æ”¯æŒLoRAå¾®è°ƒ"""
    
    def __init__(self, model_config: Optional[ModelConfig] = None, 
                 training_config: Optional[TrainingConfig] = None, 
                 data_config: Optional[DataConfig] = None):
        logger.info("ğŸš€ åˆå§‹åŒ– SFT è®­ç»ƒå™¨...")
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("âŒ transformers åº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
            logger.info("ğŸ’¡ è¯·è¿è¡Œ: pip install torch transformers datasets peft")
            raise ImportError("éœ€è¦å®‰è£… transformers ç›¸å…³åº“")
        
        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–ä¼ å…¥çš„é…ç½®
        from config import config as default_config
        self.model_config = model_config or default_config.model
        self.training_config = training_config or default_config.training
        self.data_config = data_config or default_config.data
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tokenizer = None
        self.model = None
        self.data_manager = None
        self.trainer = None
        self.train_dataset = None
        
        # ç¦ç”¨flash attention
        os.environ["DISABLE_FLASH_ATTENTION"] = "1"
        
        logger.info("âœ… SFT è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"ğŸ”§ å¼€å§‹è®¾ç½®æ¨¡å‹: {self.model_config.model_name}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            logger.info("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_config.model_name,
                trust_remote_code=self.model_config.trust_remote_code,
                use_fast=False
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                logger.debug("ğŸ”§ è®¾ç½® pad_token ä¸º eos_token")
            
            logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹
            logger.info("ğŸ¤– åŠ è½½æ¨¡å‹...")
            model_kwargs = {
                "trust_remote_code": self.model_config.trust_remote_code,
                "device_map": self.model_config.device_map,
            }
            
            # è®¾ç½®æ•°æ®ç±»å‹
            if hasattr(torch, self.model_config.torch_dtype):
                model_kwargs["torch_dtype"] = getattr(torch, self.model_config.torch_dtype)
                logger.debug(f"è®¾ç½®æ•°æ®ç±»å‹: {self.model_config.torch_dtype}")
            
            # è®¾ç½®é‡åŒ–
            if self.model_config.load_in_8bit:
                model_kwargs["load_in_8bit"] = True
                logger.info("âš¡ å¯ç”¨ 8-bit é‡åŒ–")
            elif self.model_config.load_in_4bit:
                model_kwargs["load_in_4bit"] = True
                logger.info("âš¡ å¯ç”¨ 4-bit é‡åŒ–")
            
            logger.debug(f"æ¨¡å‹å‚æ•°: {model_kwargs}")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_config.model_name,
                **model_kwargs
            )
            
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # è®¾ç½®LoRA
            if self.model_config.lora_target_modules:
                self.setup_lora()
            
            logger.info("ğŸ‰ æ¨¡å‹è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®¾ç½®å¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def setup_lora(self):
        """è®¾ç½®LoRAé…ç½®"""
        logger.info("ğŸ”§ è®¾ç½® LoRA é…ç½®...")
        
        try:
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.model_config.lora_r,
                lora_alpha=self.model_config.lora_alpha,
                lora_dropout=self.model_config.lora_dropout,
                target_modules=self.model_config.lora_target_modules,
                bias="none"
            )
            
            logger.debug(f"LoRA é…ç½®: r={self.model_config.lora_r}, alpha={self.model_config.lora_alpha}")
            logger.debug(f"ç›®æ ‡æ¨¡å—: {self.model_config.lora_target_modules}")
            
            self.model = get_peft_model(self.model, lora_config)
            
            # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
            if hasattr(self.model, 'print_trainable_parameters'):
                logger.info("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
                self.model.print_trainable_parameters()
            
            logger.info("âœ… LoRA è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ LoRA è®¾ç½®å¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“Š å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        try:
            # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
            self.data_manager = DataManager(self.data_config)
            
            # åŠ è½½æ•°æ®é›†
            logger.info("ğŸ”„ åŠ è½½æ•°æ®é›†...")
            datasets = self.data_manager.load_datasets()
            
            if not datasets:
                logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®é›†")
                raise ValueError("æ•°æ®é›†ä¸ºç©º")
            
            # åˆå¹¶æ•°æ®é›†
            logger.info("ğŸ”„ åˆå¹¶æ•°æ®é›†...")
            combined_data = self.data_manager.combine_datasets()
            
            if not combined_data:
                logger.error("âŒ åˆå¹¶åçš„æ•°æ®é›†ä¸ºç©º")
                raise ValueError("åˆå¹¶æ•°æ®é›†å¤±è´¥")
            
            # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
            logger.info("ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼...")
            train_data = self.format_data_for_training(combined_data)
            
            if not train_data:
                logger.error("âŒ æ ¼å¼åŒ–åçš„è®­ç»ƒæ•°æ®ä¸ºç©º")
                raise ValueError("æ•°æ®æ ¼å¼åŒ–å¤±è´¥")
            
            # åˆ›å»ºDatasetå¯¹è±¡
            logger.info("ğŸ”„ åˆ›å»º Dataset å¯¹è±¡...")
            self.train_dataset = Dataset.from_list(train_data)
            
            logger.info(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {len(self.train_dataset)} æ¡æ ·æœ¬")
            
            # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
            stats = self.data_manager.get_dataset_stats()
            logger.info("ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡:")
            for name, details in stats["dataset_details"].items():
                logger.info(f"   {name}: {details['record_count']} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def format_data_for_training(self, data: List[Dict]) -> List[Dict]:
        """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼"""
        logger.debug("ğŸ”„ æ ¼å¼åŒ–è®­ç»ƒæ•°æ®...")
        
        if not self.tokenizer:
            logger.error("âŒ åˆ†è¯å™¨æœªåˆå§‹åŒ–")
            return []
        
        formatted_data = []
        
        for i, item in enumerate(data):
            try:
                if 'conversations' in item:
                    # æ„å»ºå®Œæ•´çš„å¯¹è¯æ–‡æœ¬
                    conversation_text = ""
                    for turn in item['conversations']:
                        if turn.get('from') == 'human':
                            conversation_text += f"Human: {turn.get('value', '')}\n"
                        elif turn.get('from') == 'gpt':
                            conversation_text += f"Assistant: {turn.get('value', '')}\n"
                    
                    if not conversation_text.strip():
                        logger.warning(f"âš ï¸  ç©ºå¯¹è¯æ–‡æœ¬ï¼Œè·³è¿‡æ•°æ®é¡¹ {i}")
                        continue
                    
                    # åˆ†è¯å’Œæˆªæ–­
                    tokens = self.tokenizer(
                        conversation_text,
                        truncation=True,
                        max_length=self.data_config.max_length,
                        return_tensors=None,
                        add_special_tokens=True
                    )
                    
                    if len(tokens["input_ids"]) < 10:  # è·³è¿‡å¤ªçŸ­çš„åºåˆ—
                        logger.warning(f"âš ï¸  åºåˆ—å¤ªçŸ­ï¼Œè·³è¿‡æ•°æ®é¡¹ {i}")
                        continue
                    
                    formatted_data.append({
                        "input_ids": tokens["input_ids"],
                        "attention_mask": tokens["attention_mask"],
                        "labels": tokens["input_ids"].copy()  # å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œlabelså’Œinput_idsç›¸åŒ
                    })
                    
            except Exception as e:
                logger.warning(f"âš ï¸  æ ¼å¼åŒ–æ•°æ®é¡¹ {i} å¤±è´¥: {str(e)}")
                continue
        
        logger.debug(f"âœ… æ ¼å¼åŒ–å®Œæˆ: {len(formatted_data)} æ¡è®­ç»ƒæ ·æœ¬")
        return formatted_data
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        logger.info("âš™ï¸  è®¾ç½®è®­ç»ƒå‚æ•°...")
        
        try:
            if not self.model or not self.tokenizer or not self.train_dataset:
                logger.error("âŒ æ¨¡å‹ã€åˆ†è¯å™¨æˆ–æ•°æ®é›†æœªå‡†å¤‡å°±ç»ª")
                raise ValueError("è®­ç»ƒç»„ä»¶æœªå®Œå…¨åˆå§‹åŒ–")
            
            # åˆ›å»ºè®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=self.training_config.output_dir,
                num_train_epochs=self.training_config.num_train_epochs,
                per_device_train_batch_size=self.training_config.per_device_train_batch_size,
                per_device_eval_batch_size=self.training_config.per_device_eval_batch_size,
                gradient_accumulation_steps=self.training_config.gradient_accumulation_steps,
                learning_rate=self.training_config.learning_rate,
                weight_decay=self.training_config.weight_decay,
                warmup_steps=self.training_config.warmup_steps,
                lr_scheduler_type=self.training_config.lr_scheduler_type,
                save_steps=self.training_config.save_steps,
                eval_steps=self.training_config.eval_steps,
                logging_steps=self.training_config.logging_steps,
                save_total_limit=self.training_config.save_total_limit,
                load_best_model_at_end=self.training_config.load_best_model_at_end,
                fp16=self.training_config.fp16,
                bf16=self.training_config.bf16,
                gradient_checkpointing=self.training_config.gradient_checkpointing,
                dataloader_num_workers=self.training_config.dataloader_num_workers,
                evaluation_strategy=self.training_config.evaluation_strategy,
                save_strategy=self.training_config.save_strategy,
                report_to=self.training_config.report_to,
                remove_unused_columns=False,
                dataloader_pin_memory=False,
            )
            
            logger.info("ğŸ“‹ è®­ç»ƒå‚æ•°:")
            logger.info(f"   è¾“å‡ºç›®å½•: {training_args.output_dir}")
            logger.info(f"   è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
            logger.info(f"   æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
            logger.info(f"   å­¦ä¹ ç‡: {training_args.learning_rate}")
            logger.info(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {training_args.gradient_accumulation_steps}")
            
            # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
            data_collator = DataCollatorForSeq2Seq(
                tokenizer=self.tokenizer,
                model=self.model,
                padding=True,
                return_tensors="pt"
            )
            
            # åˆ›å»ºè®­ç»ƒå™¨
            self.trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer,
            )
            
            logger.info("âœ… è®­ç»ƒè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè®¾ç½®å¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        try:
            if not self.trainer:
                logger.error("âŒ è®­ç»ƒå™¨æœªåˆå§‹åŒ–")
                raise ValueError("è®­ç»ƒå™¨æœªè®¾ç½®")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(self.training_config.output_dir, exist_ok=True)
            
            # ä¿å­˜è®­ç»ƒé…ç½®
            config_path = os.path.join(self.training_config.output_dir, "training_config.json")
            self.save_training_config(config_path)
            
            # å¼€å§‹è®­ç»ƒ
            logger.info("ğŸ¯ å¯åŠ¨è®­ç»ƒå¾ªç¯...")
            train_result = self.trainer.train()
            
            # ä¿å­˜è®­ç»ƒç»“æœ
            logger.info("ğŸ’¾ ä¿å­˜è®­ç»ƒç»“æœ...")
            self.trainer.save_model()
            self.trainer.save_state()
            
            # è®°å½•è®­ç»ƒç»Ÿè®¡
            logger.info("ğŸ“Š è®­ç»ƒå®Œæˆç»Ÿè®¡:")
            logger.info(f"   è®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}")
            logger.info(f"   è®­ç»ƒæ­¥æ•°: {train_result.global_step}")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = os.path.join(self.training_config.output_dir, "final_model")
            self.trainer.model.save_pretrained(final_model_path)
            self.tokenizer.save_pretrained(final_model_path)
            
            logger.info(f"âœ… è®­ç»ƒæˆåŠŸå®Œæˆ! æ¨¡å‹ä¿å­˜è‡³: {final_model_path}")
            
            return train_result
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def save_training_config(self, config_path: str):
        """ä¿å­˜è®­ç»ƒé…ç½®"""
        logger.debug(f"ğŸ’¾ ä¿å­˜è®­ç»ƒé…ç½®åˆ°: {config_path}")
        
        try:
            config_data = {
                "model_config": {
                    "model_name": self.model_config.model_name,
                    "torch_dtype": self.model_config.torch_dtype,
                    "load_in_8bit": self.model_config.load_in_8bit,
                    "lora_r": self.model_config.lora_r,
                    "lora_alpha": self.model_config.lora_alpha,
                    "lora_target_modules": self.model_config.lora_target_modules,
                },
                "training_config": {
                    "num_train_epochs": self.training_config.num_train_epochs,
                    "per_device_train_batch_size": self.training_config.per_device_train_batch_size,
                    "learning_rate": self.training_config.learning_rate,
                    "output_dir": self.training_config.output_dir,
                },
                "data_config": {
                    "active_datasets": self.data_config.active_datasets,
                    "max_length": self.data_config.max_length,
                    "data_format": self.data_config.data_format,
                }
            }
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            logger.debug("âœ… è®­ç»ƒé…ç½®ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸  ä¿å­˜è®­ç»ƒé…ç½®å¤±è´¥: {str(e)}")
    
    def run_full_training(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. è®¾ç½®æ¨¡å‹
            logger.info("ğŸ“‹ æ­¥éª¤ 1/4: è®¾ç½®æ¨¡å‹")
            self.setup_model()
            
            # 2. å‡†å¤‡æ•°æ®
            logger.info("ğŸ“‹ æ­¥éª¤ 2/4: å‡†å¤‡æ•°æ®")
            self.prepare_data()
            
            # 3. è®¾ç½®è®­ç»ƒ
            logger.info("ğŸ“‹ æ­¥éª¤ 3/4: è®¾ç½®è®­ç»ƒ")
            self.setup_training()
            
            # 4. å¼€å§‹è®­ç»ƒ
            logger.info("ğŸ“‹ æ­¥éª¤ 4/4: å¼€å§‹è®­ç»ƒ")
            result = self.train()
            
            logger.info("ğŸ‰ å®Œæ•´è®­ç»ƒæµç¨‹æˆåŠŸå®Œæˆ!")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒæµç¨‹å¤±è´¥: {str(e)}", exc_info=True)
            raise

# ä¾¿æ·å‡½æ•°
def quick_train(model_key: Optional[str] = None, datasets: Optional[List[str]] = None, 
                epochs: int = 3, batch_size: int = 4) -> str:
    """å¿«é€Ÿè®­ç»ƒå‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨å¿«é€Ÿè®­ç»ƒ...")
    
    try:
        from config import config
        
        # è®¾ç½®é…ç½®
        if model_key:
            config.update_model(model_key)
            logger.info(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹: {model_key}")
        
        if datasets:
            config.update_datasets(datasets)
            logger.info(f"ğŸ“Š ä½¿ç”¨æ•°æ®é›†: {datasets}")
        
        # æ›´æ–°è®­ç»ƒå‚æ•°
        config.training.num_train_epochs = epochs
        config.training.per_device_train_batch_size = batch_size
        
        logger.info(f"âš™ï¸  è®­ç»ƒé…ç½®: {epochs} è½®, æ‰¹æ¬¡å¤§å° {batch_size}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model_config=config.model,
            training_config=config.training,
            data_config=config.data
        )
        
        # å¼€å§‹è®­ç»ƒ
        result = trainer.run_full_training()
        
        logger.info("âœ… å¿«é€Ÿè®­ç»ƒå®Œæˆ!")
        return config.training.output_dir
        
    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿè®­ç»ƒå¤±è´¥: {str(e)}", exc_info=True)
        return ""

def train_with_config(config_updates: Optional[Dict] = None) -> str:
    """ä½¿ç”¨è‡ªå®šä¹‰é…ç½®è®­ç»ƒ"""
    logger.info("ğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰é…ç½®è®­ç»ƒ...")
    
    try:
        from config import config
        
        # åº”ç”¨é…ç½®æ›´æ–°
        if config_updates:
            for section, updates in config_updates.items():
                if hasattr(config, section):
                    section_config = getattr(config, section)
                    for key, value in updates.items():
                        if hasattr(section_config, key):
                            setattr(section_config, key, value)
                            logger.debug(f"æ›´æ–°é…ç½®: {section}.{key} = {value}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model_config=config.model,
            training_config=config.training,
            data_config=config.data
        )
        
        # å¼€å§‹è®­ç»ƒ
        result = trainer.run_full_training()
        
        logger.info("âœ… è‡ªå®šä¹‰é…ç½®è®­ç»ƒå®Œæˆ!")
        return config.training.output_dir
        
    except Exception as e:
        logger.error(f"âŒ è‡ªå®šä¹‰é…ç½®è®­ç»ƒå¤±è´¥: {str(e)}", exc_info=True)
        return ""

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("ğŸ”§ SFT è®­ç»ƒå™¨ç¤ºä¾‹")
    
    # å¿«é€Ÿè®­ç»ƒç¤ºä¾‹
    print("\nğŸš€ å¯åŠ¨å¿«é€Ÿè®­ç»ƒ...")
    try:
        result = quick_train(
            model_key="qwen_7b",  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
            datasets=["english_adult", "chinese_shy"],
            epochs=1,
            batch_size=2
        )
        print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {result}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–åº“") 