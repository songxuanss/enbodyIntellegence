"""
æ•°æ®ç®¡ç†æ¨¡å— - ç»Ÿä¸€ç®¡ç†è®­ç»ƒæ•°æ®
"""

import json
import importlib
import importlib.util
import os
import logging
from typing import List, Dict, Any, Optional
from config import config, DataConfig

# è®¾ç½®æ—¥å¿—å™¨
logger = logging.getLogger(__name__)

class DataManager:
    """æ•°æ®ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†è®­ç»ƒæ•°æ®çš„åŠ è½½å’Œå¤„ç†"""
    
    def __init__(self, config: DataConfig):
        self.config = config
        self.project_root = os.path.dirname(os.path.abspath(__file__))
        self.sft_dir = "SFT"
        self.datasets: Dict[str, List[Dict]] = {}
        logger.info("ğŸ“Š æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_module_data(self, module_name: str) -> List[Dict]:
        """ä»Pythonæ¨¡å—æ–‡ä»¶åŠ è½½æ•°æ®"""
        logger.info(f"ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®æ¨¡å—: {module_name}")
        
        try:
            file_path = os.path.join(self.project_root, self.sft_dir, module_name)
            
            if not os.path.exists(file_path):
                logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return []
            
            logger.debug(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {file_path}")
            
            # åŠ¨æ€åŠ è½½æ¨¡å—
            spec = importlib.util.spec_from_file_location("data_module", file_path)
            if spec is None or spec.loader is None:
                logger.error(f"âŒ æ— æ³•åˆ›å»ºæ¨¡å—è§„èŒƒ: {file_path}")
                return []
                
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            
            # æŸ¥æ‰¾æ•°æ®å˜é‡
            data = None
            for attr_name in ['sft_training_data', 'sft_data', 'data', 'dataset', 'conversations', 'all_conversations']:
                if hasattr(module, attr_name):
                    data = getattr(module, attr_name)
                    logger.debug(f"âœ… æ‰¾åˆ°æ•°æ®å˜é‡: {attr_name}")
                    break
            
            if data is None:
                logger.warning(f"âš ï¸  æ¨¡å—ä¸­æœªæ‰¾åˆ°æ•°æ®å˜é‡: {module_name}")
                return []
            
            if not isinstance(data, list):
                logger.error(f"âŒ æ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨ä½†å¾—åˆ°: {type(data)}")
                return []
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            formatted_data = self.format_conversations(data, module_name)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(formatted_data)} æ¡æ•°æ®è®°å½•ä»: {module_name}")
            return formatted_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®æ¨¡å—å¤±è´¥ {module_name}: {str(e)}", exc_info=True)
            return []
    
    def format_conversations(self, data: List[Dict], source_name: str) -> List[Dict]:
        """å°†æ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€çš„å¯¹è¯æ ¼å¼"""
        logger.debug(f"ğŸ”„ æ ¼å¼åŒ–å¯¹è¯æ•°æ®: {source_name}")
        
        formatted_data = []
        
        for i, item in enumerate(data):
            try:
                if 'conversations' in item:
                    # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼
                    formatted_data.append(item)
                elif 'input' in item and 'output' in item:
                    # input/outputæ ¼å¼è½¬æ¢ä¸ºconversationsæ ¼å¼
                    formatted_item = {
                        "conversations": [
                            {"from": "human", "value": item['input']},
                            {"from": "gpt", "value": item['output']}
                        ]
                    }
                    formatted_data.append(formatted_item)
                else:
                    logger.warning(f"âš ï¸  è·³è¿‡æœªçŸ¥æ ¼å¼çš„æ•°æ®é¡¹ {i} åœ¨ {source_name}")
                    
            except Exception as e:
                logger.error(f"âŒ æ ¼å¼åŒ–æ•°æ®é¡¹ {i} å¤±è´¥ åœ¨ {source_name}: {str(e)}")
        
        logger.debug(f"âœ… æ ¼å¼åŒ–å®Œæˆ: {len(formatted_data)} æ¡è®°å½•")
        return formatted_data
    
    def validate_data_format(self, data: List[Dict], source_name: str) -> bool:
        """éªŒè¯æ•°æ®æ ¼å¼"""
        logger.debug(f"ğŸ” éªŒè¯æ•°æ®æ ¼å¼: {source_name}")
        
        if not data:
            logger.warning(f"âš ï¸  æ•°æ®ä¸ºç©º: {source_name}")
            return False
        
        try:
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            sample = data[0]
            if 'conversations' not in sample:
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦å­—æ®µ 'conversations' åœ¨æ•°æ®æº: {source_name}")
                return False
            
            # æ£€æŸ¥conversationsæ ¼å¼
            if not isinstance(sample['conversations'], list):
                logger.error(f"âŒ conversations å­—æ®µåº”ä¸ºåˆ—è¡¨: {source_name}")
                return False
            
            if sample['conversations'] and 'from' not in sample['conversations'][0]:
                logger.error(f"âŒ conversation ç¼ºå°‘ 'from' å­—æ®µ: {source_name}")
                return False
                
            logger.info(f"âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡: {source_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥ {source_name}: {str(e)}")
            return False
    
    def load_datasets(self) -> Dict[str, List[Dict]]:
        """åŠ è½½æ‰€æœ‰æŒ‡å®šçš„æ•°æ®é›†"""
        logger.info("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†...")
        
        if not self.config.active_datasets:
            logger.warning("âš ï¸  æ²¡æœ‰æŒ‡å®šæ´»è·ƒæ•°æ®é›†")
            return {}
        
        logger.info(f"ğŸ“‹ æ´»è·ƒæ•°æ®é›†: {self.config.active_datasets}")
        
        for dataset_name in self.config.active_datasets:
            try:
                logger.info(f"ğŸ”„ å¤„ç†æ•°æ®é›†: {dataset_name}")
                
                if dataset_name not in self.config.data_sources:
                    logger.error(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
                    continue
                
                module_file = self.config.data_sources[dataset_name]
                logger.debug(f"ğŸ“„ æ¨¡å—æ–‡ä»¶: {module_file}")
                
                data = self.load_module_data(module_file)
                
                if data and self.validate_data_format(data, dataset_name):
                    self.datasets[dataset_name] = data
                    logger.info(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {dataset_name} ({len(data)} æ¡è®°å½•)")
                else:
                    logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆæ•°æ®é›†: {dataset_name}")
                    
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥ {dataset_name}: {str(e)}", exc_info=True)
        
        total_records = sum(len(data) for data in self.datasets.values())
        logger.info(f"ğŸ‰ æ•°æ®é›†åŠ è½½å®Œæˆ! æ€»è®¡ {len(self.datasets)} ä¸ªæ•°æ®é›†ï¼Œ{total_records} æ¡è®°å½•")
        
        return self.datasets
    
    def combine_datasets(self) -> List[Dict]:
        """åˆå¹¶æ‰€æœ‰æ•°æ®é›†"""
        logger.info("ğŸ”„ å¼€å§‹åˆå¹¶æ•°æ®é›†...")
        
        if not self.datasets:
            logger.warning("âš ï¸  æ²¡æœ‰å·²åŠ è½½çš„æ•°æ®é›†å¯åˆå¹¶")
            self.load_datasets()
        
        combined_data = []
        
        for dataset_name, data in self.datasets.items():
            logger.debug(f"ğŸ“Š åˆå¹¶æ•°æ®é›†: {dataset_name} ({len(data)} æ¡è®°å½•)")
            combined_data.extend(data)
        
        logger.info(f"âœ… æ•°æ®é›†åˆå¹¶å®Œæˆ! æ€»è®¡ {len(combined_data)} æ¡è®°å½•")
        return combined_data
    
    def save_combined_dataset(self, output_file: Optional[str] = None) -> str:
        """ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†"""
        output_file = output_file or self.config.output_file
        logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜åˆå¹¶æ•°æ®é›†åˆ°: {output_file}")
        
        try:
            combined_data = self.combine_datasets()
            
            if not combined_data:
                logger.error("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                return ""
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_file)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(combined_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ•°æ®é›†ä¿å­˜æˆåŠŸ: {output_file} ({len(combined_data)} æ¡è®°å½•)")
            return output_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
            return ""
    
    def get_dataset_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        logger.debug("ğŸ“ˆ ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
        
        stats = {
            "total_datasets": len(self.datasets),
            "dataset_details": {},
            "total_records": 0,
            "data_sources_used": list(self.datasets.keys())
        }
        
        for name, data in self.datasets.items():
            dataset_stats = {
                "record_count": len(data),
                "avg_conversation_length": 0.0,
                "unique_roles": set()
            }
            
            # è®¡ç®—å¹³å‡å¯¹è¯é•¿åº¦å’Œè§’è‰²ç»Ÿè®¡
            total_conversations = 0
            for record in data:
                if 'conversations' in record:
                    total_conversations += len(record['conversations'])
                    for conv in record['conversations']:
                        if 'from' in conv:
                            dataset_stats["unique_roles"].add(conv['from'])
            
            if data:
                dataset_stats["avg_conversation_length"] = total_conversations / len(data)
            
            dataset_stats["unique_roles"] = list(dataset_stats["unique_roles"])
            stats["dataset_details"][name] = dataset_stats
            stats["total_records"] += len(data)
        
        logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå®Œæˆ: {stats['total_records']} æ¡æ€»è®°å½•")
        return stats
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        logger.debug("ğŸ“‹ è·å–æ•°æ®é›†ä¿¡æ¯...")
        
        info = {
            "data_sources": self.config.data_sources,
            "active_datasets": self.config.active_datasets or [],
            "available_files": {}
        }
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨çŠ¶æ€
        for key, filename in self.config.data_sources.items():
            file_path = os.path.join(self.sft_dir, filename)
            info["available_files"][key] = {
                "filename": filename,
                "exists": os.path.exists(file_path),
                "path": file_path
            }
        
        return info

# ä¾¿æ·å‡½æ•°
def quick_load_data(datasets: Optional[List[str]] = None, config: Optional[DataConfig] = None) -> Optional["DataManager"]:
    """å¿«é€ŸåŠ è½½æ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨å¿«é€Ÿæ•°æ®åŠ è½½...")
    
    try:
        if config is None:
            from config import config as default_config
            config = default_config.data
        
        if datasets:
            config.active_datasets = datasets
            logger.info(f"ğŸ“‹ ä½¿ç”¨æŒ‡å®šæ•°æ®é›†: {datasets}")
        
        manager = DataManager(config)
        datasets_loaded = manager.load_datasets()
        
        if datasets_loaded:
            logger.info("âœ… å¿«é€Ÿæ•°æ®åŠ è½½æˆåŠŸ!")
            return manager
        else:
            logger.error("âŒ å¿«é€Ÿæ•°æ®åŠ è½½å¤±è´¥!")
            return None
            
    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿæ•°æ®åŠ è½½å¼‚å¸¸: {str(e)}", exc_info=True)
        return None

def create_dataset(datasets: Optional[List[str]] = None, format_type: Optional[str] = None) -> str:
    """å¿«é€Ÿåˆ›å»ºæ•°æ®é›†"""
    logger.info("ğŸš€ å¼€å§‹åˆ›å»ºæ•°æ®é›†...")
    
    try:
        from config import config
        manager = DataManager(config.data)
        
        if datasets:
            manager.config.active_datasets = datasets
        
        output_file = manager.save_combined_dataset()
        
        if output_file:
            logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {output_file}")
        else:
            logger.error("âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥")
            
        return output_file
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºæ•°æ®é›†å¼‚å¸¸: {str(e)}", exc_info=True)
        return ""

def list_available_datasets():
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
    logger.info("ğŸ“‹ åˆ—å‡ºå¯ç”¨æ•°æ®é›†...")
    
    try:
        from config import config
        manager = DataManager(config.data)
        info = manager.get_dataset_info()
        
        print("\nğŸ“Š å¯ç”¨æ•°æ®é›†:")
        print(f"æ´»è·ƒæ•°æ®é›†: {info['active_datasets']}")
        print("\næ–‡ä»¶çŠ¶æ€:")
        
        for key, file_info in info["available_files"].items():
            status = "âœ…" if file_info["exists"] else "âŒ"
            print(f"   {key}: {file_info['filename']} {status}")
            
        logger.info("âœ… æ•°æ®é›†åˆ—è¡¨æ˜¾ç¤ºå®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ åˆ—å‡ºæ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("ğŸ”§ æ•°æ®ç®¡ç†å™¨ç¤ºä¾‹")
    list_available_datasets()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
    print("\nğŸš€ åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
    output = create_dataset(["english_adult", "chinese_shy"])
    if output:
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {output}") 