"""
æ¨¡å‹è¯„ä¼°æ¨¡å— - ç»Ÿä¸€çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°
"""

import json
import os
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from config import config

# è®¾ç½®æ—¥å¿—å™¨
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è¯„ä¼°ç›¸å…³åº“
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import numpy as np
    EVAL_LIBRARIES_AVAILABLE = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥è¯„ä¼°ç›¸å…³ä¾èµ–")
except ImportError as e:
    EVAL_LIBRARIES_AVAILABLE = False
    logger.warning(f"âš ï¸  è¯„ä¼°åº“æœªå®‰è£…: {e}")

from config import EvaluationConfig

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç±»"""
    model_path: str
    metrics: Dict[str, float]
    sample_outputs: List[Dict[str, str]]
    evaluation_time: float
    error_message: Optional[str] = None

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨ - æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self, config: Optional[EvaluationConfig] = None):
        logger.info("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨...")
        
        if not EVAL_LIBRARIES_AVAILABLE:
            logger.warning("âš ï¸  è¯„ä¼°åº“æœªå®Œå…¨å®‰è£…ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        
        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–ä¼ å…¥çš„é…ç½®
        from config import config as default_config
        self.config = config or default_config.evaluation
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.tokenizer = None
        self.current_model_path = None
        
        logger.info("âœ… æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_model(self, model_path: str):
        """åŠ è½½è¦è¯„ä¼°çš„æ¨¡å‹"""
        logger.info(f"ğŸ”„ åŠ è½½è¯„ä¼°æ¨¡å‹: {model_path}")
        
        try:
            if not EVAL_LIBRARIES_AVAILABLE:
                logger.error("âŒ è¯„ä¼°åº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
                raise ImportError("éœ€è¦å®‰è£… torch å’Œ transformers")
            
            if not os.path.exists(model_path):
                logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            logger.info("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                logger.debug("ğŸ”§ è®¾ç½® pad_token ä¸º eos_token")
            
            # åŠ è½½æ¨¡å‹
            logger.info("ğŸ¤– åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            
            self.current_model_path = model_path
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", exc_info=True)
            raise
    
    def generate_response(self, prompt: str, max_length: int = 512) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        logger.debug(f"ğŸ”„ ç”Ÿæˆå“åº”ï¼Œæç¤ºé•¿åº¦: {len(prompt)}")
        
        try:
            if not self.model or not self.tokenizer:
                logger.error("âŒ æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½")
                raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=max_length // 2  # ç•™ä¸€åŠé•¿åº¦ç»™ç”Ÿæˆ
            )
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            if hasattr(self.model, 'device'):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response = generated_text[len(prompt):].strip()
            
            logger.debug(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå“åº”é•¿åº¦: {len(response)}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}")
            return f"[ç”Ÿæˆå¤±è´¥: {str(e)}]"
    
    def calculate_perplexity(self, texts: List[str]) -> float:
        """è®¡ç®—å›°æƒ‘åº¦"""
        logger.info("ğŸ“Š è®¡ç®—å›°æƒ‘åº¦...")
        
        try:
            if not self.model or not self.tokenizer:
                logger.error("âŒ æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½")
                return float('inf')
            
            total_loss = 0.0
            total_tokens = 0
            
            for i, text in enumerate(texts[:self.config.max_eval_samples]):
                logger.debug(f"ğŸ”„ å¤„ç†æ–‡æœ¬ {i+1}/{len(texts)}")
                
                try:
                    # ç¼–ç æ–‡æœ¬
                    inputs = self.tokenizer(
                        text,
                        return_tensors="pt",
                        truncation=True,
                        max_length=512
                    )
                    
                    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    if hasattr(self.model, 'device'):
                        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                    
                    # è®¡ç®—æŸå¤±
                    with torch.no_grad():
                        outputs = self.model(**inputs, labels=inputs["input_ids"])
                        loss = outputs.loss
                    
                    total_loss += loss.item() * inputs["input_ids"].size(1)
                    total_tokens += inputs["input_ids"].size(1)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  è·³è¿‡æ–‡æœ¬ {i}: {str(e)}")
                    continue
            
            if total_tokens == 0:
                logger.warning("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬ç”¨äºè®¡ç®—å›°æƒ‘åº¦")
                return float('inf')
            
            avg_loss = total_loss / total_tokens
            perplexity = torch.exp(torch.tensor(avg_loss)).item()
            
            logger.info(f"âœ… å›°æƒ‘åº¦è®¡ç®—å®Œæˆ: {perplexity:.4f}")
            return perplexity
            
        except Exception as e:
            logger.error(f"âŒ å›°æƒ‘åº¦è®¡ç®—å¤±è´¥: {str(e)}", exc_info=True)
            return float('inf')
    
    def calculate_response_quality(self, prompts: List[str]) -> Dict[str, float]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        logger.info("ğŸ“Š è¯„ä¼°å“åº”è´¨é‡...")
        
        try:
            if not prompts:
                logger.warning("âš ï¸  æ²¡æœ‰æä¾›è¯„ä¼°æç¤º")
                return {"avg_response_length": 0.0, "response_rate": 0.0}
            
            responses = []
            valid_responses = 0
            total_length = 0
            
            for i, prompt in enumerate(prompts[:self.config.max_eval_samples]):
                logger.debug(f"ğŸ”„ è¯„ä¼°æç¤º {i+1}/{len(prompts)}")
                
                response = self.generate_response(prompt)
                responses.append(response)
                
                if response and not response.startswith("[ç”Ÿæˆå¤±è´¥"):
                    valid_responses += 1
                    total_length += len(response)
            
            # è®¡ç®—æŒ‡æ ‡
            response_rate = valid_responses / len(prompts) if prompts else 0.0
            avg_length = total_length / valid_responses if valid_responses > 0 else 0.0
            
            metrics = {
                "avg_response_length": avg_length,
                "response_rate": response_rate,
                "total_prompts": len(prompts),
                "valid_responses": valid_responses
            }
            
            logger.info(f"âœ… å“åº”è´¨é‡è¯„ä¼°å®Œæˆ: å“åº”ç‡ {response_rate:.2%}, å¹³å‡é•¿åº¦ {avg_length:.1f}")
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ å“åº”è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}", exc_info=True)
            return {"avg_response_length": 0.0, "response_rate": 0.0}
    
    def run_conversation_test(self, test_cases: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """è¿è¡Œå¯¹è¯æµ‹è¯•"""
        logger.info(f"ğŸ”„ è¿è¡Œå¯¹è¯æµ‹è¯•ï¼Œæµ‹è¯•ç”¨ä¾‹: {len(test_cases)}")
        
        try:
            results = []
            
            for i, case in enumerate(test_cases[:self.config.max_eval_samples]):
                logger.debug(f"ğŸ”„ æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}")
                
                try:
                    prompt = case.get("input", "")
                    expected = case.get("expected", "")
                    
                    if not prompt:
                        logger.warning(f"âš ï¸  è·³è¿‡ç©ºæç¤ºçš„æµ‹è¯•ç”¨ä¾‹ {i}")
                        continue
                    
                    # ç”Ÿæˆå“åº”
                    response = self.generate_response(f"Human: {prompt}\nAssistant:")
                    
                    # ç®€å•çš„ç›¸ä¼¼åº¦è¯„åˆ†ï¼ˆåŸºäºé•¿åº¦å’Œå…³é”®è¯ï¼‰
                    similarity_score = self.calculate_simple_similarity(response, expected) if expected else 0.0
                    
                    result = {
                        "case_id": i,
                        "input": prompt,
                        "expected": expected,
                        "generated": response,
                        "similarity_score": similarity_score,
                        "response_length": len(response)
                    }
                    
                    results.append(result)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  æµ‹è¯•ç”¨ä¾‹ {i} å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"âœ… å¯¹è¯æµ‹è¯•å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return results
            
        except Exception as e:
            logger.error(f"âŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {str(e)}", exc_info=True)
            return []
    
    def calculate_simple_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦"""
        try:
            if not text1 or not text2:
                return 0.0
            
            # è½¬æ¢ä¸ºå°å†™å¹¶åˆ†è¯
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            similarity = intersection / union if union > 0 else 0.0
            return similarity
            
        except Exception as e:
            logger.warning(f"âš ï¸  ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0
    
    def create_test_cases(self) -> List[Dict[str, str]]:
        """åˆ›å»ºé»˜è®¤æµ‹è¯•ç”¨ä¾‹"""
        logger.debug("ğŸ”§ åˆ›å»ºé»˜è®¤æµ‹è¯•ç”¨ä¾‹...")
        
        test_cases = [
            {
                "input": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
                "expected": "",
                "category": "greeting"
            },
            {
                "input": "è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„æ¦‚å¿µ",
                "expected": "",
                "category": "knowledge"
            },
            {
                "input": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
                "expected": "",
                "category": "advice"
            },
            {
                "input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
                "expected": "",
                "category": "casual"
            },
            {
                "input": "è¯·å†™ä¸€ä¸ªç®€å•çš„Pythonå‡½æ•°",
                "expected": "",
                "category": "coding"
            }
        ]
        
        logger.debug(f"âœ… åˆ›å»ºäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    def evaluate_model(self, model_path: str, custom_test_cases: Optional[List[Dict]] = None) -> EvaluationResult:
        """è¯„ä¼°æ¨¡å‹çš„å®Œæ•´æµç¨‹"""
        logger.info(f"ğŸ¯ å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_path}")
        
        import time
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            logger.info("ğŸ“‹ æ­¥éª¤ 1/4: åŠ è½½æ¨¡å‹")
            self.load_model(model_path)
            
            # 2. å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
            logger.info("ğŸ“‹ æ­¥éª¤ 2/4: å‡†å¤‡æµ‹è¯•ç”¨ä¾‹")
            test_cases = custom_test_cases or self.create_test_cases()
            
            # 3. è¿è¡Œè¯„ä¼°
            logger.info("ğŸ“‹ æ­¥éª¤ 3/4: è¿è¡Œè¯„ä¼°")
            
            # åŸºæœ¬æŒ‡æ ‡
            metrics = {}
            
            # å“åº”è´¨é‡è¯„ä¼°
            prompts = [case["input"] for case in test_cases]
            quality_metrics = self.calculate_response_quality(prompts)
            metrics.update(quality_metrics)
            
            # å¯¹è¯æµ‹è¯•
            conversation_results = self.run_conversation_test(test_cases)
            
            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
            similarities = [r["similarity_score"] for r in conversation_results if r.get("similarity_score") is not None]
            avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0
            metrics["avg_similarity"] = avg_similarity
            
            # å›°æƒ‘åº¦è®¡ç®—ï¼ˆä½¿ç”¨ç”Ÿæˆçš„å“åº”ï¼‰
            generated_texts = [r["generated"] for r in conversation_results if r.get("generated")]
            if generated_texts:
                perplexity = self.calculate_perplexity(generated_texts)
                metrics["perplexity"] = perplexity
            
            # 4. æ•´ç†ç»“æœ
            logger.info("ğŸ“‹ æ­¥éª¤ 4/4: æ•´ç†ç»“æœ")
            
            evaluation_time = time.time() - start_time
            
            result = EvaluationResult(
                model_path=model_path,
                metrics=metrics,
                sample_outputs=conversation_results[:5],  # ä¿å­˜å‰5ä¸ªæ ·æœ¬
                evaluation_time=evaluation_time
            )
            
            logger.info(f"ğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆ! è€—æ—¶: {evaluation_time:.2f}ç§’")
            logger.info("ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
            for metric, value in metrics.items():
                logger.info(f"   {metric}: {value}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}", exc_info=True)
            
            evaluation_time = time.time() - start_time
            return EvaluationResult(
                model_path=model_path,
                metrics={},
                sample_outputs=[],
                evaluation_time=evaluation_time,
                error_message=str(e)
            )
    
    def save_evaluation_result(self, result: EvaluationResult, output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        logger.info(f"ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_path}")
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # å‡†å¤‡ä¿å­˜æ•°æ®
            save_data = {
                "model_path": result.model_path,
                "evaluation_time": result.evaluation_time,
                "metrics": result.metrics,
                "sample_outputs": result.sample_outputs,
                "error_message": result.error_message,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # ä¿å­˜ä¸ºJSON
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… è¯„ä¼°ç»“æœä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}", exc_info=True)

# ä¾¿æ·å‡½æ•°
def quick_evaluate(model_path: str, output_path: Optional[str] = None) -> EvaluationResult:
    """å¿«é€Ÿè¯„ä¼°å‡½æ•°"""
    logger.info(f"ğŸš€ å¯åŠ¨å¿«é€Ÿè¯„ä¼°: {model_path}")
    
    try:
        evaluator = ModelEvaluator()
        result = evaluator.evaluate_model(model_path)
        
        # ä¿å­˜ç»“æœ
        if output_path:
            evaluator.save_evaluation_result(result, output_path)
        
        logger.info("âœ… å¿«é€Ÿè¯„ä¼°å®Œæˆ")
        return result
        
    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿè¯„ä¼°å¤±è´¥: {str(e)}", exc_info=True)
        return EvaluationResult(
            model_path=model_path,
            metrics={},
            sample_outputs=[],
            evaluation_time=0.0,
            error_message=str(e)
        )

def compare_models(model_paths: List[str]) -> Dict[str, EvaluationResult]:
    """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹"""
    logger.info(f"ğŸ”„ æ¯”è¾ƒ {len(model_paths)} ä¸ªæ¨¡å‹...")
    
    results = {}
    
    for i, path in enumerate(model_paths):
        logger.info(f"ğŸ“Š è¯„ä¼°æ¨¡å‹ {i+1}/{len(model_paths)}: {path}")
        
        try:
            result = quick_evaluate(path)
            results[path] = result
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ {path} è¯„ä¼°å¤±è´¥: {str(e)}")
            results[path] = EvaluationResult(
                model_path=path,
                metrics={},
                sample_outputs=[],
                evaluation_time=0.0,
                error_message=str(e)
            )
    
    logger.info("âœ… æ¨¡å‹æ¯”è¾ƒå®Œæˆ")
    return results

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("ğŸ”§ æ¨¡å‹è¯„ä¼°å™¨ç¤ºä¾‹")
    
    # æ¨¡æ‹Ÿè¯„ä¼°
    print("\nğŸ“Š åˆ›å»ºç¤ºä¾‹è¯„ä¼°...")
    evaluator = ModelEvaluator()
    
    # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
    test_cases = evaluator.create_test_cases()
    print(f"âœ… åˆ›å»ºäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # æ˜¾ç¤ºæµ‹è¯•ç”¨ä¾‹
    for i, case in enumerate(test_cases):
        print(f"   {i+1}. {case['input']} ({case['category']})")
    
    print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("from evaluation import quick_evaluate")
    print("result = quick_evaluate('./models/sft_output')")
    print("print(result.metrics)") 