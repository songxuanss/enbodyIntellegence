# é¡¹ç›®é…ç½®æ–‡ä»¶ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®
import os
import logging
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# é…ç½®æ—¥å¿—
def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None):
    """è®¾ç½®ç»Ÿä¸€çš„æ—¥å¿—é…ç½®"""
    # åˆ›å»ºæ—¥å¿—æ ¼å¼å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # è·å–æ ¹æ—¥å¿—å™¨
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # å¦‚æœæŒ‡å®šäº†æ—¥å¿—æ–‡ä»¶ï¼Œæ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½® - æ”¯æŒçµæ´»çš„æ¨¡å‹åç§°é…ç½®"""
    # æ¨¡å‹é€‰æ‹© - å¯ä»¥æ˜¯ä»»ä½•HuggingFaceæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
    model_name: str = "Qwen/Qwen2.5-72B-Instruct"
    
    # å¸¸ç”¨æ¨¡å‹å¿«æ·æ–¹å¼ï¼ˆå¯é€‰ï¼Œç”¨äºå¿«é€Ÿåˆ‡æ¢ï¼‰
    model_shortcuts = {
        "qwen_72b": "Qwen/Qwen2.5-72B-Instruct",
        "qwen_14b": "Qwen/Qwen2.5-14B-Instruct", 
        "qwen_7b": "Qwen/Qwen2.5-7B-Instruct",
        "qwen_3b": "Qwen/Qwen2.5-3B-Instruct",
        "qwen_1.5b": "Qwen/Qwen2.5-1.5B-Instruct",
        "qwen_0.5b": "Qwen/Qwen2.5-0.5B-Instruct",
        "phi3_14b": "microsoft/Phi-3-medium-4k-instruct",
        "phi3_4b": "microsoft/Phi-3-mini-4k-instruct", 
        "gemma_27b": "google/gemma-2-27b-it",
        "gemma_9b": "google/gemma-2-9b-it",
        "gemma_2b": "google/gemma-2-2b-it",
        "mixtral_8x7b": "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "llama3_8b": "meta-llama/Meta-Llama-3-8B-Instruct",
        "llama3_70b": "meta-llama/Meta-Llama-3-70B-Instruct",
        "llama3.1_8b": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "llama3.1_70b": "meta-llama/Meta-Llama-3.1-70B-Instruct",
        "llama3.2_1b": "meta-llama/Llama-3.2-1B-Instruct",
        "llama3.2_3b": "meta-llama/Llama-3.2-3B-Instruct",
        "yi_9b": "01-ai/Yi-1.5-9B-Chat",
        "yi_34b": "01-ai/Yi-1.5-34B-Chat",
        "deepseek_7b": "deepseek-ai/deepseek-coder-7b-instruct-v1.5",
        "deepseek_33b": "deepseek-ai/deepseek-coder-33b-instruct",
        "chatglm_6b": "THUDM/chatglm3-6b",
        "baichuan_7b": "baichuan-inc/Baichuan2-7B-Chat",
        "baichuan_13b": "baichuan-inc/Baichuan2-13B-Chat",
    }
    
    # æ¨¡å‹åŠ è½½é…ç½®
    torch_dtype: str = "bfloat16"
    device_map: str = "auto"
    trust_remote_code: bool = True
    load_in_8bit: bool = True
    load_in_4bit: bool = False
    
    # LoRAé…ç½®
    lora_r: int = 16
    lora_alpha: int = 64
    lora_dropout: float = 0.1
    lora_target_modules: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.lora_target_modules is None:
            # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è®¾ç½®target_modules
            if "qwen" in self.model_name.lower():
                self.lora_target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            elif "llama" in self.model_name.lower():
                self.lora_target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            elif "phi" in self.model_name.lower():
                self.lora_target_modules = ["qkv_proj", "o_proj", "gate_up_proj", "down_proj"]
            elif "gemma" in self.model_name.lower():
                self.lora_target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            else:
                self.lora_target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]

@dataclass
class DataConfig:
    """è®­ç»ƒæ•°æ®é…ç½®"""
    # æ•°æ®æºé…ç½® - åœ¨è¿™é‡Œç»Ÿä¸€ä¿®æ”¹è®­ç»ƒæ•°æ®
    data_sources = {
        "english_adult": "sft_adult_doll_eng.py",
        "chinese_shy": "sft_adult_doll_shy_chn.py", 
        "male_dominant": "sft_adult_doll_having_sex.py",
        "before_sex": "sft_adult_doll_before_sex_chn.py",
    }
    
    # å½“å‰ä½¿ç”¨çš„æ•°æ®æº
    active_datasets: Optional[List[str]] = None
    
    # æ•°æ®å¤„ç†é…ç½®
    max_length: int = 512
    train_test_split: float = 0.1
    data_format: str = "sharegpt"  # sharegpt, alpaca, chat
    
    # è¾“å‡ºæ–‡ä»¶é…ç½®
    output_file: str = "combined_sft_dataset.json"
    
    def __post_init__(self):
        if self.active_datasets is None:
            self.active_datasets = ["english_adult", "chinese_shy"]  # é»˜è®¤ä½¿ç”¨çš„æ•°æ®é›†

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # åŸºæœ¬è®­ç»ƒå‚æ•°
    output_dir: str = "./models/sft_output"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    
    # ä¼˜åŒ–å™¨é…ç½®
    learning_rate: float = 2e-5
    weight_decay: float = 0.1
    warmup_steps: int = 100
    lr_scheduler_type: str = "cosine"
    
    # ä¿å­˜å’Œæ—¥å¿—é…ç½®
    save_steps: int = 100
    eval_steps: int = 100
    logging_steps: int = 10
    save_total_limit: int = 3
    load_best_model_at_end: bool = True
    
    # è®­ç»ƒä¼˜åŒ–
    fp16: bool = False
    bf16: bool = True
    gradient_checkpointing: bool = True
    dataloader_num_workers: int = 4
    
    # è¯„ä¼°é…ç½®
    evaluation_strategy: str = "steps"
    save_strategy: str = "steps"
    report_to: str = "tensorboard"

@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""
    # è¯„ä¼°æ•°æ®é›†
    eval_datasets = {
        "conversation_quality": "eval_conversation_quality.json",
        "response_appropriateness": "eval_response_appropriateness.json", 
        "safety_check": "eval_safety_check.json",
    }
    
    # è¯„ä¼°æŒ‡æ ‡
    metrics = [
        "bleu",
        "rouge", 
        "bertscore",
        "perplexity",
        "response_length",
        "conversation_coherence"
    ]
    
    # è¯„ä¼°é…ç½®
    eval_batch_size: int = 8
    max_eval_samples: int = 100
    
    # æ¨¡å‹æ¯”è¾ƒé…ç½®
    baseline_models = [
        "base_model",  # æœªè®­ç»ƒçš„åŸºç¡€æ¨¡å‹
        "previous_checkpoint",  # ä¹‹å‰çš„æ£€æŸ¥ç‚¹
    ]

@dataclass
class LoggingConfig:
    """æ—¥å¿—é…ç½®ç±»"""
    log_level: str = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
    log_file: Optional[str] = "logs/training.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    console_output: bool = True  # æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°
    file_output: bool = True  # æ˜¯å¦è¾“å‡ºåˆ°æ–‡ä»¶
    
    def setup(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        return setup_logging(
            log_level=self.log_level,
            log_file=self.log_file if self.file_output else None
        )

class ProjectConfig:
    """é¡¹ç›®ä¸»é…ç½®ç±»"""
    def __init__(self):
        self.model = ModelConfig()
        self.data = DataConfig() 
        self.training = TrainingConfig()
        self.evaluation = EvaluationConfig()
        self.logging = LoggingConfig()
        
        # é¡¹ç›®è·¯å¾„é…ç½®
        self.project_root = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.project_root, "data")
        self.model_dir = os.path.join(self.project_root, "models")
        self.log_dir = os.path.join(self.project_root, "logs")
        self.eval_dir = os.path.join(self.project_root, "evaluation")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for dir_path in [self.data_dir, self.model_dir, self.log_dir, self.eval_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def update_model(self, model_identifier: str):
        """æ›´æ–°æ¨¡å‹ - æ”¯æŒå¿«æ·æ–¹å¼ã€å®Œæ•´æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„"""
        if model_identifier in self.model.model_shortcuts:
            # ä½¿ç”¨å¿«æ·æ–¹å¼
            self.model.model_name = self.model.model_shortcuts[model_identifier]
            print(f"âœ… æ¨¡å‹å·²åˆ‡æ¢ä¸º: {self.model.model_name} (ä½¿ç”¨å¿«æ·æ–¹å¼: {model_identifier})")
        else:
            # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
            self.model.model_name = model_identifier
            print(f"âœ… æ¨¡å‹å·²è®¾ç½®ä¸º: {self.model.model_name}")
            if "/" in model_identifier or "\\" in model_identifier:
                print("ğŸ’¡ æ£€æµ‹åˆ°å¯èƒ½æ˜¯æœ¬åœ°è·¯å¾„æˆ–å®Œæ•´æ¨¡å‹åç§°")
        
        # é‡æ–°åˆå§‹åŒ–LoRAé…ç½®
        self.model.__post_init__()
    
    def update_datasets(self, dataset_keys: List[str]):
        """å¿«é€Ÿåˆ‡æ¢è®­ç»ƒæ•°æ®"""
        valid_keys = []
        for key in dataset_keys:
            if key in self.data.data_sources:
                valid_keys.append(key)
            else:
                print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†: {key}")
        
        if valid_keys:
            self.data.active_datasets = valid_keys
            print(f"âœ… è®­ç»ƒæ•°æ®å·²æ›´æ–°ä¸º: {valid_keys}")
        else:
            print(f"å¯ç”¨æ•°æ®é›†: {list(self.data.data_sources.keys())}")
    
    def get_model_info(self):
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model.model_name,
            "available_shortcuts": list(self.model.model_shortcuts.keys()),
            "lora_config": {
                "r": self.model.lora_r,
                "alpha": self.model.lora_alpha,
                "dropout": self.model.lora_dropout,
                "target_modules": self.model.lora_target_modules
            },
            "load_config": {
                "torch_dtype": self.model.torch_dtype,
                "load_in_8bit": self.model.load_in_8bit,
                "load_in_4bit": self.model.load_in_4bit,
                "device_map": self.model.device_map
            }
        }
    
    def list_model_shortcuts(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹å¿«æ·æ–¹å¼"""
        print("ğŸ“‹ å¯ç”¨çš„æ¨¡å‹å¿«æ·æ–¹å¼:")
        print("=" * 60)
        
        # æŒ‰ç±»åˆ«åˆ†ç»„æ˜¾ç¤º
        categories = {
            "Qwenç³»åˆ—": [k for k in self.model.model_shortcuts.keys() if k.startswith("qwen")],
            "LLaMAç³»åˆ—": [k for k in self.model.model_shortcuts.keys() if k.startswith("llama")],
            "Phiç³»åˆ—": [k for k in self.model.model_shortcuts.keys() if k.startswith("phi")],
            "Gemmaç³»åˆ—": [k for k in self.model.model_shortcuts.keys() if k.startswith("gemma")],
            "å…¶ä»–æ¨¡å‹": [k for k in self.model.model_shortcuts.keys() 
                       if not any(k.startswith(prefix) for prefix in ["qwen", "llama", "phi", "gemma"])]
        }
        
        for category, models in categories.items():
            if models:
                print(f"\n{category}:")
                for model in sorted(models):
                    print(f"  {model:15} -> {self.model.model_shortcuts[model]}")
        
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("  1. ä½¿ç”¨å¿«æ·æ–¹å¼: --model qwen_7b")
        print("  2. ä½¿ç”¨å®Œæ•´åç§°: --model Qwen/Qwen2.5-7B-Instruct")
        print("  3. ä½¿ç”¨æœ¬åœ°è·¯å¾„: --model /path/to/local/model")
        print("  4. ä½¿ç”¨HuggingFaceç”¨æˆ·æ¨¡å‹: --model username/model-name")
    
    def validate_model_identifier(self, model_identifier: str) -> dict:
        """éªŒè¯å¹¶åˆ†ææ¨¡å‹æ ‡è¯†ç¬¦"""
        import os
        
        result = {
            "valid": False,
            "type": None,
            "resolved_name": None,
            "message": ""
        }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¿«æ·æ–¹å¼
        if model_identifier in self.model.model_shortcuts:
            result.update({
                "valid": True,
                "type": "shortcut",
                "resolved_name": self.model.model_shortcuts[model_identifier],
                "message": f"å¿«æ·æ–¹å¼ '{model_identifier}' -> '{self.model.model_shortcuts[model_identifier]}'"
            })
            return result
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
        if os.path.exists(model_identifier):
            result.update({
                "valid": True,
                "type": "local_path",
                "resolved_name": model_identifier,
                "message": f"æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_identifier}"
            })
            return result
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆæ³•çš„HuggingFaceæ¨¡å‹åç§°æ ¼å¼
        if "/" in model_identifier and len(model_identifier.split("/")) == 2:
            org, model = model_identifier.split("/")
            if org and model:  # ç¡®ä¿ç»„ç»‡åå’Œæ¨¡å‹åéƒ½ä¸ä¸ºç©º
                result.update({
                    "valid": True,
                    "type": "huggingface_repo",
                    "resolved_name": model_identifier,
                    "message": f"HuggingFaceæ¨¡å‹: {model_identifier}"
                })
                return result
        
        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        result["message"] = f"æ— æ•ˆçš„æ¨¡å‹æ ‡è¯†ç¬¦: {model_identifier}ã€‚è¯·ä½¿ç”¨å¿«æ·æ–¹å¼ã€å®Œæ•´çš„HuggingFaceæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ã€‚"
        return result
    
    def get_model_suggestions(self, partial_name: str) -> List[str]:
        """æ ¹æ®éƒ¨åˆ†åç§°è·å–æ¨¡å‹å»ºè®®"""
        suggestions = []
        partial_lower = partial_name.lower()
        
        # ä»å¿«æ·æ–¹å¼ä¸­æœç´¢
        for shortcut in self.model.model_shortcuts.keys():
            if partial_lower in shortcut.lower():
                suggestions.append(shortcut)
        
        return sorted(suggestions)
    
    def get_data_info(self):
        """è·å–å½“å‰æ•°æ®é…ç½®ä¿¡æ¯"""
        active_datasets = self.data.active_datasets or []
        return {
            "active_datasets": active_datasets,
            "data_sources": {k: v for k, v in self.data.data_sources.items() if k in active_datasets},
            "max_length": self.data.max_length,
            "output_file": self.data.output_file
        }

# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
config = ProjectConfig()

# å¿«é€Ÿé…ç½®å‡½æ•°
def quick_config(model_key: Optional[str] = None, datasets: Optional[List[str]] = None):
    """å¿«é€Ÿé…ç½®æ¨¡å‹å’Œæ•°æ®"""
    if model_key:
        config.update_model(model_key)
    if datasets:
        config.update_datasets(datasets)
    return config

# é¢å¤–çš„é…ç½®å·¥å…·å‡½æ•°
def list_available_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    config.list_model_shortcuts()

def validate_model(model_name: str):
    """éªŒè¯æ¨¡å‹åç§°"""
    result = config.validate_model_identifier(model_name)
    if result["valid"]:
        print(f"âœ… {result['message']}")
        return True
    else:
        print(f"âŒ {result['message']}")
        suggestions = config.get_model_suggestions(model_name)
        if suggestions:
            print(f"ğŸ’¡ ç›¸ä¼¼çš„å¿«æ·æ–¹å¼: {', '.join(suggestions[:5])}")
        return False

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ˜¾ç¤ºå½“å‰é…ç½®
    print("ğŸ”§ å½“å‰æ¨¡å‹é…ç½®:")
    print(config.get_model_info())
    
    print("\nğŸ“Š å½“å‰æ•°æ®é…ç½®:")
    print(config.get_data_info())
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    print("\n" + "="*50)
    list_available_models()
    
    # éªŒè¯æ¨¡å‹ç¤ºä¾‹
    print("\n" + "="*50)
    print("ğŸ§ª æ¨¡å‹éªŒè¯æµ‹è¯•:")
    test_models = ["qwen_7b", "invalid_model", "Qwen/Qwen2.5-7B-Instruct", "/local/path"]
    for model in test_models:
        print(f"\næµ‹è¯•: {model}")
        validate_model(model)
    
    # å¿«é€Ÿåˆ‡æ¢ç¤ºä¾‹
    # quick_config(model_key="qwen_14b", datasets=["chinese_shy", "male_dominant"]) 